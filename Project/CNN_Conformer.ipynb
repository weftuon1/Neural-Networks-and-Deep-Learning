{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN+Conformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x7TTOHp1DebI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys    \n","import os\n","path_to_module = '/content/drive/MyDrive/247pj/'\n","sys.path.append(path_to_module)\n","os.chdir(path_to_module)"]},{"cell_type":"code","source":["%matplotlib inline\n","import torch\n","import numpy as np\n","\n","# get the device, either cuda or cpu\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('device type is {}'.format(device))"],"metadata":{"id":"2vKONvaaDvu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test = np.load(\"X_test.npy\")\n","y_test = np.load(\"y_test.npy\")\n","person_train_valid = np.load(\"person_train_valid.npy\")\n","X_train_valid = np.load(\"X_train_valid.npy\")\n","y_train_valid = np.load(\"y_train_valid.npy\")\n","person_test = np.load(\"person_test.npy\")"],"metadata":{"id":"4TJrLX82Dyly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n","print ('Test data shape: {}'.format(X_test.shape))\n","print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n","print ('Test target shape: {}'.format(y_test.shape))\n","print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n","print ('Person test shape: {}'.format(person_test.shape))"],"metadata":{"id":"l_XZqAkXD-xk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize(array):\n","  return (array - np.mean(array, axis=0)) / np.std(array, axis=0)\n","#preprocess data\n","y_train_valid -= 769\n","y_test -= 769"],"metadata":{"id":"RT-Y3iKOEC9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_prep(X,y,sub_sample,average,noise):\n","    \n","    total_X = None\n","    total_y = None\n","    \n","    # Trimming the data (sample,22,1000) -> (sample,22,500)\n","    X = X[:,:,0:500]\n","    print('Shape of X after trimming:',X.shape)\n","    \n","    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n","    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n","    \n","    \n","    total_X = X_max\n","    total_y = y\n","    print('Shape of X after maxpooling:',total_X.shape)\n","    # Averaging + noise \n","    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n","    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n","    total_X = np.vstack((total_X, X_average))\n","    total_y = np.hstack((total_y, y))\n","    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n","    \n","    # Subsampling\n","    \n","    for i in range(sub_sample):\n","        \n","        X_subsample = X[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n","            \n","        total_X = np.vstack((total_X, X_subsample))\n","        total_y = np.hstack((total_y, y))\n","        \n","    \n","    print('Shape of X after subsampling and concatenating:',total_X.shape)\n","    return total_X,total_y"],"metadata":{"id":"4dGbCpWi7BwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Preprocessing the dataset\n","\n","X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True)\n","X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n","\n","print(X_train_valid_prep.shape)\n","print(y_train_valid_prep.shape)\n","print(X_test_prep.shape)\n","print(y_test_prep.shape)\n","\n","\n","\n","## Random splitting and reshaping the data\n","\n","# First generating the training and validation indices using random splitting\n","ind_valid = np.random.choice(8460, 1500, replace=False)\n","ind_train = np.array(list(set(range(8460)).difference(set(ind_valid))))\n","#subject 1\n","#ind_valid = np.random.choice(944, 150, replace=False)\n","#ind_train = np.array(list(set(range(944)).difference(set(ind_valid))))\n","\n","# Creating the training and validation sets using the generated indices\n","(x_train, x_valid) = X_train_valid_prep[ind_train], X_train_valid_prep[ind_valid] \n","(y_train, y_valid) = y_train_valid_prep[ind_train], y_train_valid_prep[ind_valid]\n","print('Shape of training set:',x_train.shape)\n","print('Shape of validation set:',x_valid.shape)\n","print('Shape of training labels:',y_train.shape)\n","print('Shape of validation labels:',y_valid.shape)"],"metadata":{"id":"K8tG7UYa7MFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","batch_size = 64\n","trainset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).type(torch.LongTensor))\n","valset = TensorDataset(torch.from_numpy(x_valid).float(), torch.from_numpy(y_valid).type(torch.LongTensor))\n","testset = TensorDataset(torch.from_numpy(X_test_prep).float(), torch.from_numpy(y_test_prep).type(torch.LongTensor))\n","\n","trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n","testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"],"metadata":{"id":"nzKAdJ4pEI2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install conformer"],"metadata":{"id":"vFPOhypJEaTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from conformer import ConformerBlock"],"metadata":{"id":"k6mK8EIJW7Eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class swap021(nn.Module):\n","  def forward(self, x):\n","    return x.permute(0, 2, 1)\n","class swap102(nn.Module):\n","  def forward(self, x):\n","    return x.permute(1, 0, 2)\n","class swap120(nn.Module):\n","  def forward(self, x):\n","    return x.permute(1, 2, 0)"],"metadata":{"id":"tuQlbuglcOf1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from conformer import ConformerBlock\n","class Classifier(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.net = nn.Sequential(\n","\n","        nn.Conv1d(22, 44, 10, padding='same'),\n","        nn.ELU(),\n","        nn.MaxPool1d(3, padding=1),\n","        nn.BatchNorm1d(44),\n","        nn.Dropout(0.6),\n","        nn.Conv1d(44, 88, 10, padding='same'),\n","        nn.ELU(),\n","        nn.MaxPool1d(3, padding=1),\n","        nn.BatchNorm1d(88),\n","        nn.Dropout(0.6),\n","        swap021(),\n","        swap102(),\n","        ConformerBlock(conv_kernel_size=10, dim=88, attn_dropout = 0.3, heads = 1,\n","                ff_dropout = 0.3, conv_dropout = 0.3),\n","        swap120(),\n","        nn.AvgPool1d(7),\n","        nn.Flatten(),\n","        nn.Linear(352, 4)\n","\n","    )\n","\n","    self.criterion = nn.CrossEntropyLoss()\n","\n","  def forward(self, x):\n","    return self.net(x)\n","  def cal_loss(self, pred, target):\n","    return self.criterion(pred, target)\n","\n","model = Classifier().to(device)"],"metadata":{"id":"X-Ad1prVEWRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","PATH_loss = './attention_loss.pth'\n","\n","def train(tr_set, model, device):\n","\n","    n_epochs = 80\n","\n","    # Setup optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    loss_record = {'train':[], 'val':[]}\n","    acc_record = {'train':[], 'val':[]}\n","    best_loss = 10\n","    best_acc = 0\n","    for epoch in range(n_epochs):\n","        model.train()\n","        for data in tr_set:\n","            optimizer.zero_grad()\n","            x, y = data\n","            x, y = x.to(device), y.to(device)\n","            pred = model(x)\n","            loss = model.cal_loss(pred, y)\n","            loss.backward()\n","            optimizer.step()\n","        \n","        acc_val, loss_val = test(valloader, model, device)\n","        acc_train, loss_train = test(trainloader, model, device)\n","        acc_record['val'].append(acc_val)\n","        loss_record['val'].append(loss_val)\n","        acc_record['train'].append(acc_train)\n","        loss_record['train'].append(loss_train)\n","        print('Finished training after {} epochs'.format(epoch+1))\n","        print('acc on validation set:{}'.format(acc_val))\n","        if loss_val < best_loss:\n","          best_loss = loss_val\n","          # Save:\n","          torch.save(model.state_dict(), PATH_loss) # save state_dict\n","    plt.subplot(2, 1, 1)\n","    plt.title('loss')\n","    plt.plot(loss_record['train'], '-', label='train')\n","    plt.plot(loss_record['val'], '-', label='val')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='upper right')\n","    plt.subplot(2, 1, 2)\n","    plt.title('Accuracy')\n","    plt.plot(acc_record['train'], '-', label='train')\n","    plt.plot(acc_record['val'], '-', label='val')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='lower right')\n","    plt.gcf().set_size_inches(15, 12)\n","    plt.show()"],"metadata":{"id":"lSSsDky9G8lp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(tt_set, model, device):\n","    correct = total = loss = 0\n","    model.eval()\n","    for x, y in tt_set:\n","      x, y = x.to(device), y.to(device)\n","      with torch.no_grad():\n","        pred = model(x)\n","        loss += model.cal_loss(pred, y).item() * y.size(0)\n","        _, predicted = torch.max(pred.data, 1)\n","        total += y.size(0)\n","        correct += (predicted == y).sum().item()\n","    acc = correct / total\n","    loss /= total\n","    return acc, loss"],"metadata":{"id":"NZy1ovY0HIdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(trainloader, model, device)"],"metadata":{"id":"N-0zOeF_5q_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_model_loss = Classifier()\n","best_model_loss.load_state_dict(torch.load(PATH_loss)) # load state_dict\n","best_model_loss.to(device)\n","best_model_loss.eval() # sets model in evaluation (inference) mode"],"metadata":{"id":"QGQbj7nIxgc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test(testloader, best_model_loss, device))\n","print(test(testloader, model, device))"],"metadata":{"id":"ffvSWxX35kPB"},"execution_count":null,"outputs":[]}]}